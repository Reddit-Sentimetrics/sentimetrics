{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e385a1be-1334-4291-9745-795edf7ead39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\nCollecting tqdm\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\nRequirement already satisfied: click in /databricks/python3/lib/python3.9/site-packages (from nltk) (8.0.3)\nCollecting regex>=2021.8.3\n  Downloading regex-2023.3.23-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (768 kB)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.9/site-packages (from nltk) (1.0.1)\nInstalling collected packages: tqdm, regex, nltk\nSuccessfully installed nltk-3.8.1 regex-2023.3.23 tqdm-4.65.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc6b2544-9ad1-4d1a-b5d2-7c2d9ac31907",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting praw\n  Downloading praw-7.7.0-py3-none-any.whl (189 kB)\nCollecting update-checker>=0.18\n  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\nCollecting prawcore<3,>=2.1\n  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\nCollecting websocket-client>=0.54.0\n  Downloading websocket_client-1.5.1-py3-none-any.whl (55 kB)\nRequirement already satisfied: requests<3.0,>=2.6.0 in /databricks/python3/lib/python3.9/site-packages (from prawcore<3,>=2.1->praw) (2.26.0)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.2)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\nInstalling collected packages: websocket-client, update-checker, prawcore, praw\nSuccessfully installed praw-7.7.0 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.5.1\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aabc712a-43f1-4727-84de-fb99cf549745",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping subreddits:  ['microsoft', 'google', 'youtube', 'intel', 'aws', 'azure', 'apple', 'raspberry_pi', 'android', 'amazon', 'openai', 'bing']\n"
     ]
    }
   ],
   "source": [
    "# Author: Jash\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def scrape_subreddits(subreddit_names):\n",
    "    \"\"\"\n",
    "    Scrapes the top 100 posts of each subreddit in subreddit_names from the past month\n",
    "\n",
    "    Args:\n",
    "        subreddit_names (list of str): List of subreddit names\n",
    "\n",
    "    Returns:\n",
    "        (list of lists): List of posts, where each post is a list of [subreddit, title, score, num_comments, created_str]\n",
    "    \"\"\"\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=\"7Ld-hmmFLnuBRncnLLmDTg\",\n",
    "        client_secret=\"rb3VXiZyK0DzY_Ih2ZnoJ464Ik1w4g\",\n",
    "        user_agent=\"Fair_Tomorrow_5835\"\n",
    "    )\n",
    "\n",
    "    posts = []\n",
    "    for name in subreddit_names:\n",
    "        subreddit = reddit.subreddit(name)\n",
    "        for post in subreddit.top(limit=100, time_filter='month'):\n",
    "            created_datetime = datetime.datetime.fromtimestamp(post.created)\n",
    "            created_str = created_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            posts.append([post.subreddit, post.title, post.score, post.num_comments, created_str])\n",
    "\n",
    "    return posts\n",
    "\n",
    "def get_posts():\n",
    "    \"\"\"\n",
    "    Main function to retrieve posts from subreddits and convert them to a Pandas dataframe\n",
    "\n",
    "    Returns:\n",
    "        (Pandas DataFrame): Dataframe of scraped subreddit posts with columns [Subreddit, Title, Score, Num_comments, Date]\n",
    "    \"\"\"\n",
    "    subreddit_names = ['microsoft', 'google', 'youtube', 'intel', 'aws', 'azure', 'apple', 'raspberry_pi', 'android', 'amazon', 'openai', 'bing']\n",
    "    print('Scraping subreddits: ', subreddit_names)\n",
    "\n",
    "    posts = scrape_subreddits(subreddit_names)\n",
    "    posts = pd.DataFrame(posts, columns=['Subreddit', 'Title', 'Score', 'Num_comments', 'Date'])\n",
    "    posts['Subreddit'] = posts['Subreddit'].astype(str)\n",
    "    return posts\n",
    "    # posts.to_csv('data/reddit_data.csv')\n",
    "\n",
    "\n",
    "posts = get_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32a1d845-866a-43ae-83fb-8957e64c629b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n+---------+--------------------+-----+------------+-------------------+--------------------+\n|Subreddit|               Title|Score|Num_comments|               Date|      Title_filtered|\n+---------+--------------------+-----+------------+-------------------+--------------------+\n|microsoft|bing gaining new ...|  392|          91|2023-03-24 16:43:23|bing gain new use...|\n|microsoft|get off my deskto...|  324|          56|2023-03-28 17:26:55|get desktop windo...|\n|microsoft|microsoft 365 cop...|  231|          64|2023-03-16 16:19:05|  microsoft  copilot|\n|microsoft|why is the ms hom...|  192|          62|2023-03-19 23:49:03|whi ms homepag fi...|\n|microsoft|i switched to edg...|  178|          43|2023-04-02 07:56:21|switch edg  year ...|\n|microsoft|bing offers free ...|  157|          13|2023-03-22 10:29:53|bing offer free i...|\n|microsoft|microsoft ranked ...|  150|          30|2023-03-13 17:20:06|microsoft rank rd...|\n|microsoft|microsoft swiftke...|  145|          28|2023-04-06 21:34:39|microsoft swiftke...|\n|microsoft|bing search engin...|  135|          10|2023-03-16 07:04:29|bing search engin...|\n|microsoft|microsoft-owned g...|  135|          34|2023-03-29 09:57:15|microsoftown gith...|\n+---------+--------------------+-----+------------+-------------------+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Author: Jash\n",
    "\n",
    "import pyspark\n",
    "import nltk\n",
    "import string\n",
    "from pyspark.sql.functions import split, col, regexp_replace, concat_ws, lower, udf\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Download stopwords and initialize SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    Preprocesses the data by removing stopwords, punctuations, and stemming the words\n",
    "\n",
    "    Args:\n",
    "        df (Pandas DataFrame): DataFrame containing the data\n",
    "\n",
    "    Returns:\n",
    "        (Spark DataFrame): DataFrame containing the processed data\n",
    "    \"\"\"\n",
    "    # Create a Spark session\n",
    "    SparkContext = pyspark.SparkContext\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    sc._conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") # Disable Arrow optimization\n",
    "    spark = pyspark.sql.SparkSession(sc)\n",
    "\n",
    "    # Create a Spark DataFrame from the Pandas DataFrame\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "\n",
    "    # Preprocess the data\n",
    "    processed_df = data_preprocess(spark_df)\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "def data_preprocess(df, stopwords=stopwords):\n",
    "    \"\"\"\n",
    "    Processes the data by removing stopwords, punctuations, and stemming the words\n",
    "\n",
    "    Args:\n",
    "        df (Spark DataFrame): DataFrame containing the data\n",
    "        stopwords (list of str): List of stopwords to remove\n",
    "\n",
    "    Returns:\n",
    "        (Spark DataFrame): DataFrame containing the processed data\n",
    "    \"\"\"\n",
    "\n",
    "    # Use a regular expression to remove all non-alphabetic characters and punctuations\n",
    "    df = df.withColumn(\"Title_clean\", regexp_replace(col(\"Title\"), r'[^a-zA-Z\\s]', ''))\n",
    "    # Convert all words to lowercase\n",
    "    df = df.withColumn(\"Title\", lower(col(\"Title\")))\n",
    "    # Split the \"Title_clean\" column into an array of words\n",
    "    df = df.withColumn(\"Title_words\", split(col(\"Title_clean\"), \" \"))\n",
    "    # Stem the array of words in the \"Title_words\" column\n",
    "    df = df.withColumn(\"Title_words\", udf(lambda x: [snowball.stem(word) for word in x], ArrayType(StringType()))(col(\"Title_words\")))\n",
    "    # Remove the stopwords case insensitively from the \"Title\" column\n",
    "    remover = StopWordsRemover(inputCol=\"Title_words\", outputCol=\"Title_filtered\", stopWords=stopwords, caseSensitive=False)\n",
    "    filtered_df = remover.transform(df)\n",
    "    # Join the words back into a sentence\n",
    "    filtered_df = filtered_df.withColumn(\"Title_filtered\", concat_ws(\" \", col(\"Title_filtered\")))\n",
    "    filtered_df = filtered_df.drop(\"Title_clean\", \"Title_words\")\n",
    "    # Lower the case of the Title_filtered column\n",
    "    filtered_df = filtered_df.withColumn(\"Title_filtered\", lower(col(\"Title_filtered\")))\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "preprocessed_posts = preprocess(posts)\n",
    "\n",
    "# Show the first 10 rows of the processed data\n",
    "preprocessed_posts.show(10)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "reddit_scrape",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
